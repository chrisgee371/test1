{"parameters":[{"name":"severity","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"log_type","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"log_message","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"table_name","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"log_table_name","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"rows_inserted","type":"integer","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"rows_updated","type":"integer","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"rows_deleted","type":"integer","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"start_dtm","type":"timestamp","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"end_dtm","type":"timestamp","metadata":{"description":"","tags":[],"mappings":[]}}],"isCustomSchemaEnabled":false,"code":"from datetime import datetime\nfrom pyspark.sql import SparkSession\n\n\ndef log_copper_load(\n    #spark: SparkSession,\n    ins_tms,\n    bus_dt,\n    extract_bus_dt,\n    src_sys,\n    severity,\n    log_type,\n    log_message,\n    wkf_name,\n    wkf_run_id,\n    task_name,\n    task_run_id,\n    copper_table=None,\n    rows_inserted=None,\n    rows_updated=None,\n    rows_deleted=None,\n    start_dtm=None,\n    end_dtm=None\n):\n    spark = SparkSession.builder.getOrCreate()\n    \n    # Convert string timestamps to datetime if still strings\n    def ensure_dt(v):\n        if v is None:\n            return None\n        if isinstance(v, datetime):\n            return v\n        return datetime.strptime(v, \"%Y-%m-%d %H:%M:%S\")\n\n    ins_tms = ensure_dt(ins_tms)\n    start_dtm = ensure_dt(start_dtm)\n    end_dtm = ensure_dt(end_dtm)\n\n    def ensure_integer(v):\n        if v is None:\n            return None\n        if isinstance(v, int):\n            return v\n        return v.cast(int)\n\n    rows_inserted = ensure_integer(rows_inserted)\n    rows_updated = ensure_integer(rows_updated)\n    rows_deleted = ensure_integer(rows_deleted)\n\n    row_vals = [(ins_tms, bus_dt, extract_bus_dt, src_sys, copper_table, severity, log_type,\n                 rows_inserted, rows_updated, rows_deleted, start_dtm, end_dtm,\n                 log_message, wkf_name, wkf_run_id, task_name, task_run_id)]\n\n    schema = \"\"\"\n      ins_tms TIMESTAMP, bus_dt STRING, extract_bus_dt STRING, src_sys STRING,\n      copper_table STRING, severity STRING, log_type STRING,\n      rows_inserted INT, rows_updated INT, rows_deleted INT,\n      start_dtm TIMESTAMP, end_dtm TIMESTAMP, log_message STRING,\n      wkf_name STRING, wkf_run_id STRING, task_name STRING, task_run_id STRING\n    \"\"\"\n\n    (spark.createDataFrame(row_vals, schema)\n          .write\n          .format(\"delta\")\n          .mode(\"append\")\n          .saveAsTable(log_table_name))\n\n# Call\n#wkf_name=Config.var_wkf_name\nwkf_name=\"executed from Prophecy\"\nlog_copper_load(\n    #spark,\n    ins_tms=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    bus_dt=\"var_bus_dt\",\n    extract_bus_dt=\"var_bus_dt\",\n    src_sys=\"TAO_SFIC_G\",\n    severity=severity,\n    log_type=log_type,\n    log_message=log_message,\n    wkf_name=\"var_wkf_name\",\n    wkf_run_id=\"var_wkf_run_id\",\n    task_name=\"var_task_name\",\n    task_run_id=\"var_task_run_id\",\n    copper_table=table_name,\n    rows_inserted=rows_inserted,\n    rows_updated=rows_updated,\n    rows_deleted=rows_deleted,\n    start_dtm=start_dtm,\n    end_dtm=end_dtm\n)","language":"python","description":""}