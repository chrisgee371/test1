{"parameters":[{"name":"raw_file_directory_path","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"file_format","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"description","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"copper_table","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"options","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"severity","type":"string","metadata":{"description":"","tags":[],"mappings":[]}},{"name":"log_type","type":"string","metadata":{"description":"","tags":[],"mappings":[]}}],"isCustomSchemaEnabled":false,"code":"from pyspark.sql import SparkSession, DataFrame\r\nfrom pyspark.sql.types import StructType\r\n#copper_log_table = f\"{Config.var_catalog_name}.{Config.var_log_container}.copper_load_log\"\r\ncopper_log_table = \"hrdp_catalog_dev.log.copper_load_log\"\r\n\r\ndef safe_read(\r\n        #spark: SparkSession,\r\n        path: str,\r\n        *,\r\n        fmt: str = \"csv\",\r\n        description: str = \"\",\r\n        copper_table: str | None = None,\r\n        options: dict | None = None,\r\n        schema: StructType | None = None,\r\n        severity: str = \"ERROR\",\r\n        log_type: str = \"GENERIC\",\r\n        eval_rows: int = 1,\r\n        on_error: str = \"empty\",  # \"empty\" | \"none\" | \"raise\"\r\n        logger = None\r\n) -> DataFrame | None:\r\n\r\n    spark = SparkSession.builder.getOrCreate()\r\n    \"\"\"\r\n    Safely read a dataset with minimal evaluation.\r\n    \"\"\"\r\n    options = options or {}\r\n\r\n    if logger is None:\r\n        def logger(sev, ltype, msg, copper_table=None)->str:\r\n            print(f\"[{sev}][{ltype}][{copper_table}] {msg}\")\r\n            return(f\"[{sev}][{ltype}][{copper_table}] {msg}\")\r\n\r\n    try:\r\n        reader = spark.read.format(fmt)\r\n\r\n        for k, v in options.items():\r\n            reader = reader.option(k, v)\r\n\r\n        df = reader.schema(schema).load(path) if schema else reader.load(path)\r\n\r\n        if eval_rows > 0:\r\n            df.limit(eval_rows).count() # cheap validation\r\n\r\n        log_message = logger(\"INFO\", log_type, f\"Success: {description}\", copper_table)\r\n        #log_message = logger(Config.var_info_type, log_type, f\"Success: {description}\", copper_table)\r\n        #log_message = f\"Read raw file for {copper_table} \"\r\n        log_message = description\r\n        tao_custom_logger(\r\n            #Config.var_info_type,\r\n            \"INFO\",\r\n            #Config.var_file_check,\r\n            \"FILE_CHECK\",\r\n            log_message,\r\n            copper_table,\r\n            copper_log_table,\r\n            None,\r\n            None,\r\n            None,\r\n            None,\r\n            None\r\n        )\r\n        return df\r\n    except Exception as e:\r\n        #log_message = logger(severity, log_type, f\"Error during read: {description}: {e}\", copper_table)\r\n        log_message = \"Error during -> \" + log_message + str(e)\r\n        tao_custom_logger(\r\n            #Config.var_error_type,\r\n            \"ERROR\",\r\n            #Config.var_file_check,\r\n            \"FILE_CHECK\",\r\n            log_message,\r\n            copper_table,\r\n            copper_log_table,\r\n            None,\r\n            None,\r\n            None,\r\n            None,\r\n            None\r\n        )\r\n        if on_error == \"raise\":\r\n            raise \r\n\r\n        if on_error == \"empty\":\r\n\r\n            if schema:\r\n                return spark.createDataFrame([], schema)\r\n\r\n            return spark.createDataFrame([], [])\r\n\r\n        return None\r\n\r\ndf = safe_read(\r\n    #spark,\r\n    raw_file_directory_path,\r\n    fmt = file_format,\r\n    description = description,\r\n    copper_table = copper_table,\r\n    options = options,\r\n    severity = severity,\r\n    log_type = log_type,\r\n    on_error = \"empty\"  # or \"none\" or \"raise\"\r\n)\r\n\r\nreturn df","language":"python","description":""}